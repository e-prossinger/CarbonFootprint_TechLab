{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":29047,"sourceType":"datasetVersion","datasetId":22655}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/mahboobehabdighara/semanticsegmentation-techlabsproject-ipynb?scriptVersionId=223961624\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Load & Display Data","metadata":{}},{"cell_type":"code","source":"import os  #os: Handles file paths & directory access\nimport numpy as np\nimport cv2   # cv2:  (OpenCV) Loads and processes images\nimport matplotlib.pyplot as plt\n\n# Set dataset path\nDATASET_PATH = \"/kaggle/input/cityscapes-image-pairs/cityscapes_data/train\"\n\n# Load an image\ndef load_and_display_image(idx=0):\n    files = sorted(os.listdir(DATASET_PATH))  # Get sorted file list --> ensures that images and masks match up correctly\n                                              # os.listdir(DATASET_PATH) â†’ Lists all image files in the dataset folder\n                                    #sorted() â†’ Ensures images are processed in order (prevents randomness).\n                                    #files[idx] â†’ Selects the idx-th image from the sorted list.\n                                    #os.path.join(DATASET_PATH, files[idx]) â†’ Gets the full file path of the selected image.\n    img_path = os.path.join(DATASET_PATH, files[idx])\n    \n    # Read image\n    img = cv2.imread(img_path)    #Loads the image as a NumPy array\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # Convert from BGR to RGB -->OpenCV loads images in BGR format, but Matplotlib expects RGB. This fixes color issues\n    \n    # Split into left (original) and right (mask)\n    h, w, _ = img.shape   # Gets the height (h), width (w), and channels (_) of the image\n    img_left = img[:, :w//2, :]   # Original Image -->Selects the left half of the image (original)\n    img_right = img[:, w//2:, :]  # Segmentation Mask --> Selects the right half (segmentation mask)\n\n    # Show both images\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5)) #Creates two side-by-side images.\n\n    ax1.imshow(img_left) #Displays the original image\n    ax1.set_title(\"Original Image\")\n    ax1.axis(\"off\")\n\n    ax2.imshow(img_right)  #Displays the segmentation mask\n    ax2.set_title(\"Segmentation Mask\")\n    ax2.axis(\"off\")  #Hides axis labels for a cleaner view\n\n    plt.show()\n\n# Display first image\nload_and_display_image(5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:51:59.416241Z","iopub.execute_input":"2025-02-18T09:51:59.416548Z","iopub.status.idle":"2025-02-18T09:52:00.32027Z","shell.execute_reply.started":"2025-02-18T09:51:59.416514Z","shell.execute_reply":"2025-02-18T09:52:00.319254Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preprocess Data; resize, normalize, and prepare the dataset for training.","metadata":{}},{"cell_type":"code","source":"IMAGE_SIZE = (256, 256)  # Increase resolution for better results--> helps standardize input images so the model processes them consistently\n\ndef load_images(dataset_path, image_size):\n    image_files = sorted(os.listdir(dataset_path))  # Sort image filenames\n    images, masks = [], [] #empty lists to store processed images & masks\n\n    for img_name in image_files:\n        img_path = os.path.join(dataset_path, img_name) # Creates the full path of the image file\n        img = cv2.imread(img_path) #Reads the image as a NumPy array\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) #Converts BGR â†’ RGB (since OpenCV loads images in BGR format)\n\n        # Split into left (image) and right (mask)\n        h, w, _ = img.shape\n        img_left = img[:, :w//2, :]   # Original Image\n        img_right = img[:, w//2:, :]  # Segmentation Mask\n\n        # Resize\n        img_left = cv2.resize(img_left, image_size)\n        img_right = cv2.resize(img_right, image_size)\n        #Resizes both the original image and mask to (256, 256).\n        #Ensures all images are the same size for deep learning models.\n\n        \n        # Normalize images (scale between 0 and 1); makes training more stable and faster.\n        img_left = img_left / 255.0  \n        \n        # Convert mask to grayscale\n        img_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY) #Converts the mask to grayscale (from RGB)\n        img_right = cv2.resize(img_right, image_size)\n        img_right = img_right / 255.0  # Normalize mask, between 0 and 1\n        #Why Convert to Grayscale?\n\n#The segmentation mask only contains 2 values (object or background), so we donâ€™t need color channels.\n#Reduces memory usage and speeds up training.\n\n        images.append(img_left) #Stores the processed image & mask in their respective lists\n        masks.append(img_right)\n\n    return np.array(images), np.array(masks)\n    #Converts lists to NumPy arrays, which are needed for deep learning models\n\n# Load dataset\nX, Y = load_images(DATASET_PATH, IMAGE_SIZE) #Calls load_images() to load and preprocess all images.\n#Stores the processed images in X and masks in Y.\nY = Y.reshape(Y.shape[0], Y.shape[1], Y.shape[2], 1)  # Ensure correct shape; Ensures Y (masks) has a 4D shape\n\nprint(f\"âœ… Loaded {len(X)} images and masks.\")\nprint(f\"ðŸ“ Image shape: {X.shape}, Mask shape: {Y.shape}\")\n#Prints how many images and masks were loaded.\n#Displays the shape of images & masks to confirm correctness.\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:52:09.565812Z","iopub.execute_input":"2025-02-18T09:52:09.566125Z","iopub.status.idle":"2025-02-18T09:52:53.319368Z","shell.execute_reply.started":"2025-02-18T09:52:09.5661Z","shell.execute_reply":"2025-02-18T09:52:53.318613Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Split into Training & Validation","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\n# Split dataset\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n#Splits X (images) & Y (masks) into:\n#X_train, Y_train â†’ 80% for training.\n#X_val, Y_val â†’ 20% for validation.\n\n\n# Print dataset sizes\nprint(f\"ðŸ”¹ Training set: {X_train.shape}, {Y_train.shape}\")\nprint(f\"ðŸ”¹ Validation set: {X_val.shape}, {Y_val.shape}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:53:04.867062Z","iopub.execute_input":"2025-02-18T09:53:04.867351Z","iopub.status.idle":"2025-02-18T09:53:07.071781Z","shell.execute_reply.started":"2025-02-18T09:53:04.86733Z","shell.execute_reply":"2025-02-18T09:53:07.070885Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Build a U-Net\nðŸ”¹ Use Batch Normalization for stability\nðŸ”¹ Use Dropout to reduce overfitting\nðŸ”¹ More filters for better feature extraction","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers, models\n#tensorflow.keras.layers â†’ Provides deep learning layers (Conv2D, MaxPooling2D, etc.).\n#tensorflow.keras.models â†’ Allows us to define and build a model.\n\ndef build_unet(input_shape=(256, 256, 3)): #creates a U-Net model; model expects RGB images of size 256x256\n    inputs = layers.Input(shape=input_shape) #Creates the input layer for the network\n\n    # Encoder\n    conv1 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(inputs)\n    #Conv2D (64, 3, activation=\"relu\", padding=\"same\")\n    #Applies 64 convolutional filters of size 3x3.\n    #ReLU activation helps learn complex patterns.\n    #Padding=\"same\" ensures the output size remains the same.\n    \n    conv1 = layers.BatchNormalization()(conv1)  # Stabilize training\n    conv1 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(conv1)\n    pool1 = layers.MaxPooling2D(pool_size=(2, 2))(conv1)\n    #MaxPooling2D (2x2) â†’ Reduces spatial size by half (downsampling)\n\n    conv2 = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(pool1)\n    #Uses 128 filters for deeper feature extraction\n    conv2 = layers.BatchNormalization()(conv2)\n    conv2 = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(conv2)\n    pool2 = layers.MaxPooling2D(pool_size=(2, 2))(conv2)\n    #Starts as (256x256) â†’ after 1st pooling â†’ (128x128)\n    #After 2nd pooling â†’ (64x64)\n    \n\n    # Bottleneck\n    conv3 = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(pool2) #256 filters â†’ Captures high-level abstract features\n    conv3 = layers.Dropout(0.3)(conv3)  # Dropout (0.3) â†’ Randomly disables 30% of neurons to prevent overfitting\n\n    \n    # Decoder-upsampling to original size\n    up4 = layers.Conv2DTranspose(128, 3, strides=2, activation=\"relu\", padding=\"same\")(conv3)\n    #Upsamples (doubles size) from (64x64) to (128x128)\n    merge4 = layers.concatenate([conv2, up4], axis=3) #Combines decoder output with encoder feature maps\n    conv4 = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(merge4) #Refines details after upsampling\n\n    up5 = layers.Conv2DTranspose(64, 3, strides=2, activation=\"relu\", padding=\"same\")(conv4)\n    merge5 = layers.concatenate([conv1, up5], axis=3)\n    conv5 = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(merge5)\n\n    # Output layer\n    outputs = layers.Conv2D(1, 1, activation=\"sigmoid\")(conv5)\n\n    model = models.Model(inputs, outputs) #Creates the full U-Net model\n    return model\n\n# Create model\nunet_model = build_unet()\nunet_model.summary() #Prints model architecture (number of layers, parameters, etc.)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:53:16.346323Z","iopub.execute_input":"2025-02-18T09:53:16.346799Z","iopub.status.idle":"2025-02-18T09:53:30.385273Z","shell.execute_reply.started":"2025-02-18T09:53:16.34677Z","shell.execute_reply":"2025-02-18T09:53:30.384598Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Add Dice Loss & IoU Metric","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\n# Dice Loss (better for segmentation)\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true = K.cast(y_true, dtype=\"float32\")\n    y_pred = K.cast(y_pred, dtype=\"float32\")\n    #Converts y_true and y_pred to float32 to ensure compatible types for operations\n\n    y_true = K.flatten(y_true)\n    y_pred = K.flatten(y_pred)\n    #Flattens tensors from (batch_size, height, width, channels) to a 1D array.\n\n    intersection = K.sum(y_true * y_pred) #Computes the intersection (overlap) between predicted and true masks\n    return 1 - (2. * intersection + smooth) / (K.sum(y_true) + K.sum(y_pred) + smooth)\n    #Small constant added to prevent division by zero and improve numerical stability\n\n# IoU Score (for evaluation)--> Intersection over Union\ndef iou_score(y_true, y_pred, smooth=1e-6):\n    y_true = K.cast(y_true, dtype=\"float32\") #Ensures both y_true and y_pred are float32 to prevent type mismatch\n    y_pred = K.cast(y_pred, dtype=\"float32\")\n\n    y_true = K.flatten(y_true)\n    y_pred = K.flatten(y_pred)\n\n    intersection = K.sum(y_true * y_pred) #Counts the pixels where both masks are 1 (overlap)\n    union = K.sum(y_true) + K.sum(y_pred) - intersection #Counts all the pixels that belong to either the predicted or true mask\n\n    return (intersection + smooth) / (union + smooth)\n\n#Dice Loss--> Lower is better (perfect = 0)\n#IoU-->Higher is better (perfect = 1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:53:42.180654Z","iopub.execute_input":"2025-02-18T09:53:42.181214Z","iopub.status.idle":"2025-02-18T09:53:42.188315Z","shell.execute_reply.started":"2025-02-18T09:53:42.181187Z","shell.execute_reply":"2025-02-18T09:53:42.187637Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Compile the Model with:\nAdam optimizer (adaptive learning) & Dice Loss (better for segmentation) & IoU as a metric","metadata":{}},{"cell_type":"code","source":"# Compile with Dice Loss & IoU metric\nunet_model.compile(optimizer=\"adam\", loss=dice_loss, metrics=[\"accuracy\", iou_score])\n#Optimizer â†’ Controls how the model updates its weights during training.\n#Loss Function â†’ Measures how far the modelâ€™s predictions are from the actual values.\n#Metrics â†’ Additional evaluation metrics to monitor the modelâ€™s performance.\n\n\n\n#customised version:\n#from tensorflow.keras.optimizers import Adam\n\n#unet_model.compile(optimizer=Adam(learning_rate=0.0001), loss=dice_loss, metrics=[\"accuracy\", iou_score])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:53:56.755208Z","iopub.execute_input":"2025-02-18T09:53:56.755505Z","iopub.status.idle":"2025-02-18T09:53:56.76882Z","shell.execute_reply.started":"2025-02-18T09:53:56.755484Z","shell.execute_reply":"2025-02-18T09:53:56.768117Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Apply Data Augmentation for Better Generalization\nTo improve model accuracy and robustness, we use augmentation like:\n\nFlipping images horizontally; Rotating up to 20 degrees; Changing brightness","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\n#Data augmentation is a technique to artificially expand the dataset by applying \n#random transformations (e.g., flipping, rotating, etc.) to the input images.\n\n# Create an ImageDataGenerator for augmentation\ndatagen = ImageDataGenerator(\n    horizontal_flip=True,\n    rotation_range=20,\n    brightness_range=[0.8, 1.2]\n)\n\n# Apply augmentation to training data\ntrain_gen = datagen.flow(X_train, Y_train, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:54:05.36691Z","iopub.execute_input":"2025-02-18T09:54:05.367203Z","iopub.status.idle":"2025-02-18T09:54:06.096428Z","shell.execute_reply.started":"2025-02-18T09:54:05.367181Z","shell.execute_reply":"2025-02-18T09:54:06.095478Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### train the U-Net model for 30+ epochs with augmentation","metadata":{}},{"cell_type":"code","source":"history = unet_model.fit(train_gen, validation_data=(X_val, Y_val), epochs=30, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T09:54:17.490583Z","iopub.execute_input":"2025-02-18T09:54:17.490881Z","iopub.status.idle":"2025-02-18T10:13:29.81091Z","shell.execute_reply.started":"2025-02-18T09:54:17.490859Z","shell.execute_reply":"2025-02-18T10:13:29.810169Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### IoU > 0.5 â†’ Good segmentation!\nIoU ~ 0.3-0.5 â†’ Decent but needs improvement.\nIoU < 0.3 â†’ The model is not learning well.\n\n","metadata":{}},{"cell_type":"markdown","source":"### Plot Loss & IoU","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot loss over epochs\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot IoU over epochs\nplt.subplot(1, 2, 2)\nplt.plot(history.history['iou_score'], label='Train IoU')\nplt.plot(history.history['val_iou_score'], label='Val IoU')\nplt.title('IoU Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\nplt.legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:13:56.23405Z","iopub.execute_input":"2025-02-18T10:13:56.234404Z","iopub.status.idle":"2025-02-18T10:13:56.580476Z","shell.execute_reply.started":"2025-02-18T10:13:56.234375Z","shell.execute_reply":"2025-02-18T10:13:56.579663Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Predictions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random #to select random images from the validation set\n\n# Select random test images\nnum_samples = 10\nfig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n#plt.subplots() creates a grid of images to display.\n#num_samples, 3 means 10 rows and 3 columns (original, true mask, predicted mask).\n#figsize=(12, 4 * num_samples) defines the figure size for better clarity.\n\nfor i in range(num_samples):\n    idx = random.randint(0, len(X_val) - 1) #picks a random index from the validation set\n    test_image = X_val[idx] #stores the input image\n    true_mask = Y_val[idx] #stores the corresponding ground truth mask\n\n    # Make a prediction\n    predicted_mask = unet_model.predict(test_image[np.newaxis, ...])[0]\n    predicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n    #test_image[np.newaxis, ...] adds a batch dimension (required by Keras).\n    #.predict() generates the predicted mask.\n    ##Thresholding: Converts probabilities to binary (0 or 1):\n    #> 0.5 â†’ Foreground (object).\n    #<= 0.5 â†’ Background.\n\n    # Plot original image\n    axes[i, 0].imshow(test_image) #imshow() displays the original image\n    axes[i, 0].set_title(\"Original Image\") #labels the image\n    axes[i, 0].axis(\"off\") #removes axis ticks and labels for clarity\n\n    # Plot ground truth mask\n    axes[i, 1].imshow(true_mask[:, :, 0], cmap=\"gray\") #displays the actual mask\n    #cmap=\"gray\" renders the mask in grayscale\n    axes[i, 1].set_title(\"Ground Truth Mask\")\n    axes[i, 1].axis(\"off\")\n\n    # Plot predicted mask\n    axes[i, 2].imshow(predicted_mask[:, :, 0], cmap=\"gray\")\n    axes[i, 2].set_title(\"Predicted Mask\")\n    axes[i, 2].axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:15:13.613043Z","iopub.execute_input":"2025-02-18T10:15:13.613365Z","iopub.status.idle":"2025-02-18T10:15:17.497221Z","shell.execute_reply.started":"2025-02-18T10:15:13.61334Z","shell.execute_reply":"2025-02-18T10:15:17.49603Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Improved Code; Version two","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# Set dataset path\nDATASET_PATH = \"/kaggle/input/cityscapes-image-pairs/cityscapes_data/train\"\n\n# Image size\nIMAGE_SIZE = (128, 128)\n\ndef load_images(dataset_path, image_size):\n    image_files = sorted(os.listdir(dataset_path))\n    images, masks = [], []\n\n    for img_name in image_files:\n        img_path = os.path.join(dataset_path, img_name)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Split into left (image) and right (mask)\n        h, w, _ = img.shape\n        img_left = img[:, :w//2, :]\n        img_right = img[:, w//2:, :]\n\n        # Resize\n        img_left = cv2.resize(img_left, image_size)\n        img_right = cv2.resize(img_right, image_size)\n\n        # Normalize images\n        img_left = img_left / 255.0  \n        \n        # Convert mask to grayscale\n        img_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n        img_right = cv2.resize(img_right, image_size)\n        img_right = img_right / 255.0\n\n        images.append(img_left)\n        masks.append(img_right)\n\n    return np.array(images), np.array(masks)\n\n# Load dataset\nX, Y = load_images(DATASET_PATH, IMAGE_SIZE)\nY = Y.reshape(Y.shape[0], Y.shape[1], Y.shape[2], 1)  # Ensure correct shape\n\n# Split dataset\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nprint(f\"âœ… Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:16:50.201234Z","iopub.execute_input":"2025-02-18T10:16:50.201595Z","iopub.status.idle":"2025-02-18T10:17:01.320356Z","shell.execute_reply.started":"2025-02-18T10:16:50.201545Z","shell.execute_reply":"2025-02-18T10:17:01.319301Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import tensorflow as tf\n\n# Define number of output classes (1 for binary segmentation)\nnum_classes = 1\n\ndef get_unet_model():\n    inputs = tf.keras.layers.Input(shape=(128, 128, 3))\n\n    # First Downsample\n    f1 = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(inputs)\n    b1 = tf.keras.layers.BatchNormalization()(f1)\n    f2 = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(b1)  # Used later for residual connection\n\n    m3 = tf.keras.layers.MaxPooling2D((2, 2))(f2)\n    d4 = tf.keras.layers.Dropout(0.2)(m3)\n\n    # Second Downsample\n    f5 = tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(d4)\n    b5 = tf.keras.layers.BatchNormalization()(f5)\n    f6 = tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(b5)\n\n    m7 = tf.keras.layers.MaxPooling2D((2, 2))(f6)\n    d8 = tf.keras.layers.Dropout(0.2)(m7)\n\n    # Third Downsample\n    f9 = tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(d8)\n    b9 = tf.keras.layers.BatchNormalization()(f9)\n    f10 = tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(b9)\n\n    m11 = tf.keras.layers.MaxPooling2D((2, 2))(f10)\n    d12 = tf.keras.layers.Dropout(0.2)(m11)\n\n    # Fourth Downsample\n    f13 = tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(d12)\n    b13 = tf.keras.layers.BatchNormalization()(f13)\n    f14 = tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(b13)\n\n    m15 = tf.keras.layers.MaxPooling2D((2, 2))(f14)\n    d16 = tf.keras.layers.Dropout(0.2)(m15)\n\n    # Fifth Downsample\n    f17 = tf.keras.layers.Conv2D(1024, (3, 3), padding=\"same\", activation=\"relu\")(d16)\n    b17 = tf.keras.layers.BatchNormalization()(f17)\n    f18 = tf.keras.layers.Conv2D(1024, (3, 3), padding=\"same\", activation=\"relu\")(b17)\n\n    # First Upsample\n    m19 = tf.keras.layers.UpSampling2D((2, 2))(f18)\n    d19 = tf.keras.layers.Dropout(0.2)(m19)\n    c20 = tf.keras.layers.Concatenate()([d19, f14])\n    f21 = tf.keras.layers.Conv2D(512, (3, 3), padding=\"same\", activation=\"relu\")(c20)\n    b21 = tf.keras.layers.BatchNormalization()(f21)\n    f22 = tf.keras.layers.Conv2D(512, ((3, 3)), padding=\"same\", activation=\"relu\")(b21)\n\n    # Second Upsample\n    m23 = tf.keras.layers.UpSampling2D((2, 2))(f22)\n    d23 = tf.keras.layers.Dropout(0.2)(m23)\n    c24 = tf.keras.layers.Concatenate()([d23, f10])\n    f25 = tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(c24)\n    b25 = tf.keras.layers.BatchNormalization()(f25)\n    f26 = tf.keras.layers.Conv2D(256, (3, 3), padding=\"same\", activation=\"relu\")(b25)\n\n    # Third Upsample\n    m27 = tf.keras.layers.UpSampling2D((2, 2))(f26)\n    d27 = tf.keras.layers.Dropout(0.2)(m27)\n    c28 = tf.keras.layers.Concatenate()([d27, f6])\n    f29 = tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(c28)\n    b29 = tf.keras.layers.BatchNormalization()(f29)\n    f30 = tf.keras.layers.Conv2D(128, (3, 3), padding=\"same\", activation=\"relu\")(b29)\n\n    # Fourth Upsample\n    m31 = tf.keras.layers.UpSampling2D((2, 2))(f30)\n    d31 = tf.keras.layers.Dropout(0.2)(m31)\n    c32 = tf.keras.layers.Concatenate()([d31, f2])\n    f33 = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(c32)\n    b33 = tf.keras.layers.BatchNormalization()(f33)\n    f34 = tf.keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation=\"relu\")(b33)\n\n    # Output Layer - Using Sigmoid for Binary Segmentation\n    outputs = tf.keras.layers.Conv2D(num_classes, (1, 1), activation=\"sigmoid\")(f34)\n\n    model = tf.keras.Model(inputs=inputs, outputs=outputs)\n    return model\n\n# Create model\nunet_model = get_unet_model()\nunet_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:17:09.187405Z","iopub.execute_input":"2025-02-18T10:17:09.187755Z","iopub.status.idle":"2025-02-18T10:17:09.475898Z","shell.execute_reply.started":"2025-02-18T10:17:09.187724Z","shell.execute_reply":"2025-02-18T10:17:09.475234Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Compile & Train the Model","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\n# Dice Loss Function\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true = K.flatten(K.cast(y_true, dtype=\"float32\"))\n    y_pred = K.flatten(K.cast(y_pred, dtype=\"float32\"))\n\n    intersection = K.sum(y_true * y_pred)\n    return 1 - (2. * intersection + smooth) / (K.sum(y_true) + K.sum(y_pred) + smooth)\n\n# IoU Metric\ndef iou_score(y_true, y_pred, smooth=1e-6):\n    y_true = K.flatten(K.cast(y_true, dtype=\"float32\"))\n    y_pred = K.flatten(K.cast(y_pred, dtype=\"float32\"))\n\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred) - intersection\n    return (intersection + smooth) / (union + smooth)\n\n# Compile Model\nunet_model.compile(optimizer=\"adam\", loss=dice_loss, metrics=[\"accuracy\", iou_score])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:19:36.172701Z","iopub.execute_input":"2025-02-18T10:19:36.173062Z","iopub.status.idle":"2025-02-18T10:19:36.184131Z","shell.execute_reply.started":"2025-02-18T10:19:36.173035Z","shell.execute_reply":"2025-02-18T10:19:36.183228Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = unet_model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=30, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:19:46.129519Z","iopub.execute_input":"2025-02-18T10:19:46.129899Z","iopub.status.idle":"2025-02-18T10:32:34.083431Z","shell.execute_reply.started":"2025-02-18T10:19:46.129873Z","shell.execute_reply":"2025-02-18T10:32:34.082682Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## results \nIoU = 0.3677 â†’ The model is correctly segmenting some regions but can be improved.\nLoss (~ 0.46) â†’ Acceptable but can be reduced further.\nAccuracy (~ 0.02) â†’ Not useful for segmentation, can be ignored.","metadata":{}},{"cell_type":"markdown","source":"## Plot Loss & IoU Over Epochs","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot loss over epochs\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\n# Plot IoU over epochs\nplt.subplot(1, 2, 2)\nplt.plot(history.history['iou_score'], label='Train IoU')\nplt.plot(history.history['val_iou_score'], label='Val IoU')\nplt.title('IoU Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\nplt.legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:33:14.324678Z","iopub.execute_input":"2025-02-18T10:33:14.325017Z","iopub.status.idle":"2025-02-18T10:33:14.648771Z","shell.execute_reply.started":"2025-02-18T10:33:14.324989Z","shell.execute_reply":"2025-02-18T10:33:14.647919Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Code to Display Predictions","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport random\n\n# Select random test images\nnum_samples = 30\nfig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n\nfor i in range(num_samples):\n    idx = random.randint(0, len(X_val) - 1)\n    test_image = X_val[idx]\n    true_mask = Y_val[idx]\n\n    # Make a prediction\n    predicted_mask = unet_model.predict(test_image[np.newaxis, ...])[0]\n    predicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n\n    # Plot original image\n    axes[i, 0].imshow(test_image)\n    axes[i, 0].set_title(\"Original Image\")\n    axes[i, 0].axis(\"off\")\n\n    # Plot ground truth mask\n    axes[i, 1].imshow(true_mask[:, :, 0], cmap=\"gray\")\n    axes[i, 1].set_title(\"Ground Truth Mask\")\n    axes[i, 1].axis(\"off\")\n\n    # Plot predicted mask\n    axes[i, 2].imshow(predicted_mask[:, :, 0], cmap=\"gray\")\n    axes[i, 2].set_title(\"Predicted Mask\")\n    axes[i, 2].axis(\"off\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:35:04.315014Z","iopub.execute_input":"2025-02-18T10:35:04.315385Z","iopub.status.idle":"2025-02-18T10:35:15.570599Z","shell.execute_reply.started":"2025-02-18T10:35:04.315356Z","shell.execute_reply":"2025-02-18T10:35:15.569199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Third version","metadata":{}},{"cell_type":"markdown","source":"### Improved Model Code","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras.backend as K\n\n# Define Dice Loss (Better for Segmentation)\ndef dice_loss(y_true, y_pred, smooth=1e-6):\n    y_true = K.flatten(K.cast(y_true, dtype=\"float32\"))\n    y_pred = K.flatten(K.cast(y_pred, dtype=\"float32\"))\n\n    intersection = K.sum(y_true * y_pred)\n    return 1 - (2. * intersection + smooth) / (K.sum(y_true) + K.sum(y_pred) + smooth)\n\n# Define IoU Metric\ndef iou_score(y_true, y_pred, smooth=1e-6):\n    y_true = K.flatten(K.cast(y_true, dtype=\"float32\"))\n    y_pred = K.flatten(K.cast(y_pred, dtype=\"float32\"))\n\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred) - intersection\n    return (intersection + smooth) / (union + smooth)\n\n# ðŸ”¹ Improved Model Function\ndef create_model():\n    inp = Input(shape=(128, 128, 3))  # âœ… Fixed input shape\n\n    # Encoder (Downsampling)\n    x1 = BatchNormalization()(inp)\n    x1 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x1)\n    x1 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x1)\n    p1 = MaxPooling2D((2, 2))(x1)\n    p1 = Dropout(0.2)(p1)  # âœ… Added dropout for regularization\n\n    x2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p1)\n    x2 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x2)\n    p2 = MaxPooling2D((2, 2))(x2)\n    p2 = Dropout(0.2)(p2)\n\n    x3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p2)\n    x3 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x3)\n    p3 = MaxPooling2D((2, 2))(x3)\n    p3 = Dropout(0.2)(p3)\n\n    x4 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(p3)\n    x4 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x4)\n\n    # Decoder (Upsampling)\n    x5 = UpSampling2D((2, 2))(x4)\n    x5 = concatenate([x3, x5])\n    x5 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x5)\n    x5 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x5)\n\n    x6 = UpSampling2D((2, 2))(x5)\n    x6 = concatenate([x2, x6])\n    x6 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x6)\n    x6 = Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\")(x6)\n\n    x7 = UpSampling2D((2, 2))(x6)\n    x7 = concatenate([x1, x7])\n    x7 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x7)\n    x7 = Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\")(x7)\n\n    # Output Layer (Binary Segmentation)\n    outputs = Conv2D(1, (1, 1), activation=\"sigmoid\")(x7)  # âœ… Changed to sigmoid for binary segmentation\n\n    # Compile Model\n    model = Model(inp, outputs)\n    opt = Adam(learning_rate=0.0001)\n    model.compile(optimizer=opt, loss=dice_loss, metrics=[\"accuracy\", iou_score])  # âœ… Using Dice Loss + IoU\n\n    return model\n\n# Create and print summary\nmodel = create_model()\nmodel.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:38:12.795593Z","iopub.execute_input":"2025-02-18T10:38:12.795933Z","iopub.status.idle":"2025-02-18T10:38:12.969883Z","shell.execute_reply.started":"2025-02-18T10:38:12.795908Z","shell.execute_reply":"2025-02-18T10:38:12.969022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Preprocess Data & Load Dataset","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport cv2\nfrom sklearn.model_selection import train_test_split\n\n# Set dataset path\nDATASET_PATH = \"/kaggle/input/cityscapes-image-pairs/cityscapes_data/train\"\n\n# Image size\nIMAGE_SIZE = (128, 128)\n\ndef load_images(dataset_path, image_size):\n    image_files = sorted(os.listdir(dataset_path))\n    images, masks = [], []\n\n    for img_name in image_files:\n        img_path = os.path.join(dataset_path, img_name)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Split into left (image) and right (mask)\n        h, w, _ = img.shape\n        img_left = img[:, :w//2, :]\n        img_right = img[:, w//2:, :]\n\n        # Resize\n        img_left = cv2.resize(img_left, image_size)\n        img_right = cv2.resize(img_right, image_size)\n\n        # Normalize images\n        img_left = img_left / 255.0  \n        \n        # Convert mask to grayscale\n        img_right = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n        img_right = cv2.resize(img_right, image_size)\n        img_right = img_right / 255.0\n\n        images.append(img_left)\n        masks.append(img_right)\n\n    return np.array(images), np.array(masks)\n\n# Load dataset\nX, Y = load_images(DATASET_PATH, IMAGE_SIZE)\nY = Y.reshape(Y.shape[0], Y.shape[1], Y.shape[2], 1)  # Ensure correct shape\n\n# Split dataset\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nprint(f\"âœ… Loaded {len(X_train)} training images and {len(X_val)} validation images\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:39:04.562555Z","iopub.execute_input":"2025-02-18T10:39:04.563116Z","iopub.status.idle":"2025-02-18T10:39:15.606452Z","shell.execute_reply.started":"2025-02-18T10:39:04.56291Z","shell.execute_reply":"2025-02-18T10:39:15.605544Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Train the Model","metadata":{}},{"cell_type":"code","source":"history = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=30, batch_size=8)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:39:24.411146Z","iopub.execute_input":"2025-02-18T10:39:24.411439Z","iopub.status.idle":"2025-02-18T10:45:48.551915Z","shell.execute_reply.started":"2025-02-18T10:39:24.411417Z","shell.execute_reply":"2025-02-18T10:45:48.551108Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Plot Training Performance","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 2, 1)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Val Loss')\nplt.title('Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.subplot(1, 2, 2)\nplt.plot(history.history['iou_score'], label='Train IoU')\nplt.plot(history.history['val_iou_score'], label='Val IoU')\nplt.title('IoU Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('IoU Score')\nplt.legend()\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:47:30.819087Z","iopub.execute_input":"2025-02-18T10:47:30.819439Z","iopub.status.idle":"2025-02-18T10:47:31.183175Z","shell.execute_reply.started":"2025-02-18T10:47:30.819411Z","shell.execute_reply":"2025-02-18T10:47:31.182325Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Visualize Model Predictions","metadata":{}},{"cell_type":"code","source":"idx = np.random.randint(0, len(X_val))\ntest_image = X_val[idx]\ntrue_mask = Y_val[idx]\npredicted_mask = model.predict(test_image[np.newaxis, ...])[0]\npredicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n\nplt.figure(figsize=(12, 4))\nplt.subplot(1, 3, 1)\nplt.imshow(test_image)\nplt.title(\"Original Image\")\n\nplt.subplot(1, 3, 2)\nplt.imshow(true_mask[:, :, 0], cmap=\"gray\")\nplt.title(\"Ground Truth Mask\")\n\nplt.subplot(1, 3, 3)\nplt.imshow(predicted_mask[:, :, 0], cmap=\"gray\")\nplt.title(\"Predicted Mask\")\n\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:48:33.450639Z","iopub.execute_input":"2025-02-18T10:48:33.450954Z","iopub.status.idle":"2025-02-18T10:48:35.023701Z","shell.execute_reply.started":"2025-02-18T10:48:33.450931Z","shell.execute_reply":"2025-02-18T10:48:35.022811Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\n# Select 20 random test images\nnum_samples = 20\nfig, axes = plt.subplots(num_samples, 3, figsize=(12, 4 * num_samples))\n\nfor i in range(num_samples):\n    idx = np.random.randint(0, len(X_val))\n    test_image = X_val[idx]\n    true_mask = Y_val[idx]\n    \n    # Predict mask\n    predicted_mask = model.predict(test_image[np.newaxis, ...])[0]\n    predicted_mask = (predicted_mask > 0.5).astype(np.uint8)\n\n    # Plot original image\n    axes[i, 0].imshow(test_image)\n    axes[i, 0].set_title(f\"Original Image {i+1}\")\n    axes[i, 0].axis(\"off\")\n\n    # Plot ground truth mask\n    axes[i, 1].imshow(true_mask[:, :, 0], cmap=\"gray\")\n    axes[i, 1].set_title(\"Ground Truth Mask\")\n    axes[i, 1].axis(\"off\")\n\n    # Plot predicted mask\n    axes[i, 2].imshow(predicted_mask[:, :, 0], cmap=\"gray\")\n    axes[i, 2].set_title(\"Predicted Mask\")\n    axes[i, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:49:13.896025Z","iopub.execute_input":"2025-02-18T10:49:13.896347Z","iopub.status.idle":"2025-02-18T10:49:22.111527Z","shell.execute_reply.started":"2025-02-18T10:49:13.896325Z","shell.execute_reply":"2025-02-18T10:49:22.109992Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4th Version, UNET Multiclass-Segmentation","metadata":{}},{"cell_type":"markdown","source":"### Step 1","metadata":{}},{"cell_type":"code","source":"# Step 1: Import Required Libraries\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\nimport cv2\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:20.919734Z","iopub.execute_input":"2025-02-18T10:51:20.920073Z","iopub.status.idle":"2025-02-18T10:51:20.924655Z","shell.execute_reply.started":"2025-02-18T10:51:20.920048Z","shell.execute_reply":"2025-02-18T10:51:20.923901Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## step 2","metadata":{}},{"cell_type":"code","source":"# Step 2: Set Constants\nDATASET_PATH = '/kaggle/input/cityscapes-image-pairs/cityscapes_data/train'\nIMAGE_SIZE = (256, 256)\nNUM_CLASSES = 5  # Adjust based on how many classes you want to segment","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:28.157787Z","iopub.execute_input":"2025-02-18T10:51:28.158081Z","iopub.status.idle":"2025-02-18T10:51:28.162174Z","shell.execute_reply.started":"2025-02-18T10:51:28.158059Z","shell.execute_reply":"2025-02-18T10:51:28.161355Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 3; it gets RAM error message.","metadata":{}},{"cell_type":"code","source":"#Output: RAM Error Message\n# Step 3: Load and Process Dataset\ndef load_images(dataset_path, image_size):\n    image_files = sorted(os.listdir(dataset_path))\n    images, masks = [], []\n\n    for img_name in image_files:\n        img_path = os.path.join(dataset_path, img_name)\n        img = cv2.imread(img_path)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n        # Split into left (image) and right (mask)\n        h, w, _ = img.shape\n        img_left = img[:, :w // 2, :]\n        img_right = img[:, w // 2:, :]\n\n        # Resize\n        img_left = cv2.resize(img_left, image_size)\n        img_right = cv2.resize(img_right, image_size)\n\n        # Normalize input image\n        img_left = img_left / 255.0\n\n        # Convert mask to classes (simulate multi-class labels)\n        mask = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n        mask = cv2.resize(mask, image_size)\n\n        # Simulate multi-class (we divide intensity ranges)\n        mask = np.digitize(mask, bins=[51, 102, 153, 204])  # Map grayscale to 5 classes\n\n        images.append(img_left)\n        masks.append(mask)\n\n    images = np.array(images)\n    masks = np.array(masks)\n\n    # One-hot encode masks\n    masks = tf.keras.utils.to_categorical(masks, num_classes=NUM_CLASSES)\n\n    return images, masks\n\n# Load dataset\nX, Y = load_images(DATASET_PATH, IMAGE_SIZE)\nX_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=42)\n\nprint(f\"âœ… Loaded {X_train.shape[0]} training images and {X_val.shape[0]} validation images.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T10:51:45.163648Z","iopub.execute_input":"2025-02-18T10:51:45.163973Z","execution_failed":"2025-02-18T10:52:19.715Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"## Step 3: Load and Process Dataset\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.utils import to_categorical\nimport cv2\nimport os\nimport random\n\n# Constants\nIMAGE_SIZE = (256, 256)\nNUM_CLASSES = 5\nBATCH_SIZE = 8\n\n# âœ… Define the dataset path\nDATASET_PATH = \"/kaggle/input/cityscapes-image-pairs/cityscapes_data/train\"\n\n# Data Generator\nclass CityscapesGenerator(tf.keras.utils.Sequence):\n    def __init__(self, dataset_path, image_size, batch_size, num_classes, mode=\"train\", split_ratio=0.8):\n        self.dataset_path = dataset_path\n        self.image_size = image_size\n        self.batch_size = batch_size\n        self.num_classes = num_classes\n        self.mode = mode\n\n        # Get image file paths\n        self.image_files = sorted(os.listdir(dataset_path))\n        random.shuffle(self.image_files)\n\n        # Split train/validation\n        split_point = int(len(self.image_files) * split_ratio)\n        if self.mode == \"train\":\n            self.image_files = self.image_files[:split_point]\n        else:\n            self.image_files = self.image_files[split_point:]\n\n    def __len__(self):\n        return int(np.ceil(len(self.image_files) / self.batch_size))\n\n    def __getitem__(self, index):\n        batch_files = self.image_files[index * self.batch_size:(index + 1) * self.batch_size]\n        images, masks = [], []\n\n        for img_name in batch_files:\n            img_path = os.path.join(self.dataset_path, img_name)\n            img = cv2.imread(img_path)\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n\n            # Split image into left (input) and right (mask)\n            h, w, _ = img.shape\n            img_left = img[:, :w // 2, :]\n            img_right = img[:, w // 2:, :]\n\n            # Resize images\n            img_left = cv2.resize(img_left, self.image_size)\n            img_right = cv2.resize(img_right, self.image_size)\n\n            # Normalize input images (scale between 0 and 1)\n            img_left = img_left / 255.0\n\n            # Convert mask to grayscale and map to classes\n            mask = cv2.cvtColor(img_right, cv2.COLOR_RGB2GRAY)\n            mask = np.digitize(mask, bins=[51, 102, 153, 204])  # Convert grayscale to classes\n            mask = to_categorical(mask, num_classes=self.num_classes)\n\n            images.append(img_left)\n            masks.append(mask)\n\n        return np.array(images), np.array(masks)\n\n# Create Generators\ntrain_generator = CityscapesGenerator(DATASET_PATH, IMAGE_SIZE, BATCH_SIZE, NUM_CLASSES, mode=\"train\")\nval_generator = CityscapesGenerator(DATASET_PATH, IMAGE_SIZE, BATCH_SIZE, NUM_CLASSES, mode=\"val\")\n\nprint(f\"âœ… Data Generators Ready: {len(train_generator)} batches for training, {len(val_generator)} for validation.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:02:10.617762Z","iopub.execute_input":"2025-02-18T11:02:10.618198Z","iopub.status.idle":"2025-02-18T11:02:10.690197Z","shell.execute_reply.started":"2025-02-18T11:02:10.618167Z","shell.execute_reply":"2025-02-18T11:02:10.689215Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 4","metadata":{}},{"cell_type":"code","source":"# Step 4: Build U-Net Model for Multi-Class Segmentation\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers, models\n\ndef build_unet(input_shape=(256, 256, 3), num_classes=NUM_CLASSES):\n    inputs = layers.Input(shape=input_shape)\n\n    # Encoder\n    def encoder_block(x, filters):\n        x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n        x = layers.BatchNormalization()(x)\n        x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n        p = layers.MaxPooling2D((2, 2))(x)\n        return p, x\n\n    p1, c1 = encoder_block(inputs, 64)\n    p2, c2 = encoder_block(p1, 128)\n\n    # Bottleneck\n    b = layers.Conv2D(256, 3, activation='relu', padding='same')(p2)\n    b = layers.Dropout(0.3)(b)\n\n    # Decoder\n    def decoder_block(x, skip, filters):\n        x = layers.Conv2DTranspose(filters, 3, strides=2, padding='same', activation='relu')(x)\n        x = layers.concatenate([x, skip])\n        x = layers.Conv2D(filters, 3, activation='relu', padding='same')(x)\n        return x\n\n    d1 = decoder_block(b, c2, 128)\n    d2 = decoder_block(d1, c1, 64)\n\n    # Output Layer (Softmax for Multi-Class Segmentation)\n    outputs = layers.Conv2D(num_classes, 1, activation='softmax')(d2)\n\n    model = models.Model(inputs, outputs)\n    return model\n\nunet_model = build_unet()\nunet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\nunet_model.summary()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:05:46.921313Z","iopub.execute_input":"2025-02-18T11:05:46.921767Z","iopub.status.idle":"2025-02-18T11:05:49.227746Z","shell.execute_reply.started":"2025-02-18T11:05:46.921734Z","shell.execute_reply":"2025-02-18T11:05:49.227023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Checking GPU availability","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nprint(\"Number of available GPUs: \", len(tf.config.list_physical_devices('GPU')))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:05:04.998958Z","iopub.execute_input":"2025-02-17T14:05:04.999596Z","iopub.status.idle":"2025-02-17T14:05:12.859096Z","shell.execute_reply.started":"2025-02-17T14:05:04.999564Z","shell.execute_reply":"2025-02-17T14:05:12.858264Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nprint(torch.cuda.is_available())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T14:03:14.513137Z","iopub.execute_input":"2025-02-17T14:03:14.513547Z","iopub.status.idle":"2025-02-17T14:03:18.165437Z","shell.execute_reply.started":"2025-02-17T14:03:14.513514Z","shell.execute_reply":"2025-02-17T14:03:18.16445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 5","metadata":{}},{"cell_type":"code","source":"# Step 5: Train the Model\n# Train the model using data generators\nhistory = unet_model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=30,\n    steps_per_epoch=len(train_generator),\n    validation_steps=len(val_generator)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:30:41.528609Z","iopub.execute_input":"2025-02-18T11:30:41.529026Z","iopub.status.idle":"2025-02-18T11:39:44.118176Z","shell.execute_reply.started":"2025-02-18T11:30:41.528999Z","shell.execute_reply":"2025-02-18T11:39:44.117362Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 6","metadata":{}},{"cell_type":"code","source":"# Step 6: Define Function to Visualize Predictions\ncolors = [\n    (255, 0, 0),    # Class 0: Red\n    (0, 255, 0),    # Class 1: Green\n    (0, 0, 255),    # Class 2: Blue\n    (255, 255, 0),  # Class 3: Yellow\n    (255, 0, 255)   # Class 4: Magenta\n]\n\ndef decode_segmentation(mask):\n    h, w, c = mask.shape\n    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n\n    for i in range(NUM_CLASSES):\n        color_mask[mask[:, :, i] == 1] = colors[i]\n\n    return color_mask\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:40:09.553389Z","iopub.execute_input":"2025-02-18T11:40:09.553798Z","iopub.status.idle":"2025-02-18T11:40:09.558845Z","shell.execute_reply.started":"2025-02-18T11:40:09.553766Z","shell.execute_reply":"2025-02-18T11:40:09.557904Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Step 7","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Ensure this function is defined to convert class indices to RGB\ndef decode_segmentation(mask):\n    colors = [\n        (255, 0, 0),    # Class 0: Red\n        (0, 255, 0),    # Class 1: Green\n        (0, 0, 255),    # Class 2: Blue\n        (255, 255, 0),  # Class 3: Yellow\n        (255, 0, 255)   # Class 4: Magenta\n    ]\n\n    h, w, c = mask.shape\n    color_mask = np.zeros((h, w, 3), dtype=np.uint8)\n\n    for i in range(NUM_CLASSES):\n        color_mask[mask[:, :, i] == 1] = colors[i]\n\n    return color_mask\n\n# Display Predictions Using the Generator\nnum_samples = 5\nfig, axes = plt.subplots(num_samples, 3, figsize=(12, 5 * num_samples))\n\nfor i in range(num_samples):\n    # Randomly select a batch from the validation generator\n    test_image, true_mask = val_generator[np.random.randint(0, len(val_generator))]\n\n    # Take the first sample from the batch\n    test_image = test_image[0]\n    true_mask = true_mask[0]\n\n    # Predict mask\n    pred_mask = unet_model.predict(test_image[np.newaxis, ...])[0]\n    pred_mask = (pred_mask == pred_mask.max(axis=-1, keepdims=True)).astype(np.uint8)\n\n    # Visualize results\n    axes[i, 0].imshow(test_image)\n    axes[i, 0].set_title(\"Original Image\")\n    axes[i, 0].axis(\"off\")\n\n    axes[i, 1].imshow(decode_segmentation(true_mask))\n    axes[i, 1].set_title(\"Ground Truth Mask\")\n    axes[i, 1].axis(\"off\")\n\n    axes[i, 2].imshow(decode_segmentation(pred_mask))\n    axes[i, 2].set_title(\"Predicted Mask\")\n    axes[i, 2].axis(\"off\")\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:49:55.439657Z","iopub.execute_input":"2025-02-18T11:49:55.440113Z","iopub.status.idle":"2025-02-18T11:49:58.79002Z","shell.execute_reply.started":"2025-02-18T11:49:55.440078Z","shell.execute_reply":"2025-02-18T11:49:58.788599Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Loss function and IoU","metadata":{}},{"cell_type":"code","source":"import tensorflow.keras.backend as K\n\ndef iou_score(y_true, y_pred, smooth=1e-6):\n    y_true = K.cast(y_true, dtype='float32')\n    y_pred = K.cast(y_pred, dtype='float32')\n\n    intersection = K.sum(y_true * y_pred)\n    union = K.sum(y_true) + K.sum(y_pred) - intersection\n\n    return (intersection + smooth) / (union + smooth)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:57:29.948932Z","iopub.execute_input":"2025-02-18T11:57:29.949241Z","iopub.status.idle":"2025-02-18T11:57:29.953516Z","shell.execute_reply.started":"2025-02-18T11:57:29.949219Z","shell.execute_reply":"2025-02-18T11:57:29.952667Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"unet_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', iou_score])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:57:36.194227Z","iopub.execute_input":"2025-02-18T11:57:36.194659Z","iopub.status.idle":"2025-02-18T11:57:36.20518Z","shell.execute_reply.started":"2025-02-18T11:57:36.194623Z","shell.execute_reply":"2025-02-18T11:57:36.204231Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Train the U-Net model with IoU metric\nhistory = unet_model.fit(\n    train_generator,\n    validation_data=val_generator,\n    epochs=30,\n    steps_per_epoch=len(train_generator),\n    validation_steps=len(val_generator)\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T11:58:29.297003Z","iopub.execute_input":"2025-02-18T11:58:29.297333Z","iopub.status.idle":"2025-02-18T12:07:44.225611Z","shell.execute_reply.started":"2025-02-18T11:58:29.297309Z","shell.execute_reply":"2025-02-18T12:07:44.224568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Function to plot training history\ndef plot_history(history):\n    # Extract metrics from history\n    loss = history.history['loss']\n    val_loss = history.history.get('val_loss', [])\n    iou = history.history['iou_score']\n    val_iou = history.history.get('val_iou_score', [])\n\n    epochs = range(1, len(loss) + 1)\n    val_epochs = range(1, len(val_loss) + 1)  # Match validation length\n\n    # Plot Loss\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, 'r', label='Training Loss')\n    if val_loss:\n        plt.plot(val_epochs, val_loss, 'b', label='Validation Loss')\n    plt.title('Loss Over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n\n    # Plot IoU Score\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, iou, 'r', label='Training IoU')\n    if val_iou:\n        plt.plot(val_epochs, val_iou, 'b', label='Validation IoU')\n    plt.title('IoU Over Epochs')\n    plt.xlabel('Epochs')\n    plt.ylabel('IoU Score')\n    plt.legend()\n\n    plt.show()\n\n# Call the plot function\nplot_history(history)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-18T12:12:14.105331Z","iopub.execute_input":"2025-02-18T12:12:14.105779Z","iopub.status.idle":"2025-02-18T12:12:14.476959Z","shell.execute_reply.started":"2025-02-18T12:12:14.105743Z","shell.execute_reply":"2025-02-18T12:12:14.476027Z"}},"outputs":[],"execution_count":null}]}