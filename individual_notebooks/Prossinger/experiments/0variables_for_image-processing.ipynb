{"cells":[{"cell_type":"markdown","id":"cdef88d4","metadata":{},"source":["# what variables will we use for image-processing"]},{"cell_type":"markdown","id":"49d87e60","metadata":{},"source":["takeaway/resume: \n","In the questionnaire,\n","* some answers can be answered per uploading a picture instead of answering per text:\n","1. 'Body Type'\n","2. ,'Sex'\n","3. ,'Diet'\n","4. ,'Heating Energy Source'\n","5. ,'Transport'\n","6. ,'Vehicle Type'\n","7. ,'Waste Bag Size'\n","8. ,'Waste Bag Weekly Count'\n","9. ,'How Many New Clothes Monthly'\n","10. 'Energy Efficiency'\n","11. ,'Recycling'\n","12. ,'Cooking_With'\n","* the answer can NOT be answered per uploading a picture:\n","13. how often Shower\n","14. Social-Activity\n","15. how often Air-Travel\n","16. Grocery_Bill\n","17. Vehicle_Distance\n","18. TV/day\n","19. Internet/day\n","\n","But if i only use Variables 1-12. the R2-score is really bad (56%), so we will have to do a mix of pictures and text-answering."]},{"cell_type":"markdown","id":"f1b3bd9d","metadata":{},"source":["### Importing the dataset and libraries"]},{"cell_type":"markdown","id":"3346de84","metadata":{},"source":["\n","we have the data from: https://www.kaggle.com/datasets/dumanmesut/individual-carbon-footprint-calculation/data"]},{"cell_type":"code","execution_count":1,"id":"6da02cfa","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Body Type</th>\n","      <th>Sex</th>\n","      <th>Diet</th>\n","      <th>How Often Shower</th>\n","      <th>Heating Energy Source</th>\n","      <th>Transport</th>\n","      <th>Vehicle Type</th>\n","      <th>Social Activity</th>\n","      <th>Monthly Grocery Bill</th>\n","      <th>Frequency of Traveling by Air</th>\n","      <th>Vehicle Monthly Distance Km</th>\n","      <th>Waste Bag Size</th>\n","      <th>Waste Bag Weekly Count</th>\n","      <th>How Long TV PC Daily Hour</th>\n","      <th>How Many New Clothes Monthly</th>\n","      <th>How Long Internet Daily Hour</th>\n","      <th>Energy efficiency</th>\n","      <th>Recycling</th>\n","      <th>Cooking_With</th>\n","      <th>CarbonEmission</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>overweight</td>\n","      <td>female</td>\n","      <td>pescatarian</td>\n","      <td>daily</td>\n","      <td>coal</td>\n","      <td>public</td>\n","      <td>NaN</td>\n","      <td>often</td>\n","      <td>230</td>\n","      <td>frequently</td>\n","      <td>210</td>\n","      <td>large</td>\n","      <td>4</td>\n","      <td>7</td>\n","      <td>26</td>\n","      <td>1</td>\n","      <td>No</td>\n","      <td>['Metal']</td>\n","      <td>['Stove', 'Oven']</td>\n","      <td>2238</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>obese</td>\n","      <td>female</td>\n","      <td>vegetarian</td>\n","      <td>less frequently</td>\n","      <td>natural gas</td>\n","      <td>walk/bicycle</td>\n","      <td>NaN</td>\n","      <td>often</td>\n","      <td>114</td>\n","      <td>rarely</td>\n","      <td>9</td>\n","      <td>extra large</td>\n","      <td>3</td>\n","      <td>9</td>\n","      <td>38</td>\n","      <td>5</td>\n","      <td>No</td>\n","      <td>['Metal']</td>\n","      <td>['Stove', 'Microwave']</td>\n","      <td>1892</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>overweight</td>\n","      <td>male</td>\n","      <td>omnivore</td>\n","      <td>more frequently</td>\n","      <td>wood</td>\n","      <td>private</td>\n","      <td>petrol</td>\n","      <td>never</td>\n","      <td>138</td>\n","      <td>never</td>\n","      <td>2472</td>\n","      <td>small</td>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>47</td>\n","      <td>6</td>\n","      <td>Sometimes</td>\n","      <td>['Metal']</td>\n","      <td>['Oven', 'Microwave']</td>\n","      <td>2595</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>overweight</td>\n","      <td>male</td>\n","      <td>omnivore</td>\n","      <td>twice a day</td>\n","      <td>wood</td>\n","      <td>walk/bicycle</td>\n","      <td>NaN</td>\n","      <td>sometimes</td>\n","      <td>157</td>\n","      <td>rarely</td>\n","      <td>74</td>\n","      <td>medium</td>\n","      <td>3</td>\n","      <td>20</td>\n","      <td>5</td>\n","      <td>7</td>\n","      <td>Sometimes</td>\n","      <td>['Paper', 'Plastic', 'Glass', 'Metal']</td>\n","      <td>['Microwave', 'Grill', 'Airfryer']</td>\n","      <td>1074</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>obese</td>\n","      <td>female</td>\n","      <td>vegetarian</td>\n","      <td>daily</td>\n","      <td>coal</td>\n","      <td>private</td>\n","      <td>diesel</td>\n","      <td>often</td>\n","      <td>266</td>\n","      <td>very frequently</td>\n","      <td>8457</td>\n","      <td>large</td>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>5</td>\n","      <td>6</td>\n","      <td>Yes</td>\n","      <td>['Paper']</td>\n","      <td>['Oven']</td>\n","      <td>4743</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["    Body Type     Sex         Diet How Often Shower Heating Energy Source  \\\n","0  overweight  female  pescatarian            daily                  coal   \n","1       obese  female   vegetarian  less frequently           natural gas   \n","2  overweight    male     omnivore  more frequently                  wood   \n","3  overweight    male     omnivore      twice a day                  wood   \n","4       obese  female   vegetarian            daily                  coal   \n","\n","      Transport Vehicle Type Social Activity  Monthly Grocery Bill  \\\n","0        public          NaN           often                   230   \n","1  walk/bicycle          NaN           often                   114   \n","2       private       petrol           never                   138   \n","3  walk/bicycle          NaN       sometimes                   157   \n","4       private       diesel           often                   266   \n","\n","  Frequency of Traveling by Air  Vehicle Monthly Distance Km Waste Bag Size  \\\n","0                    frequently                          210          large   \n","1                        rarely                            9    extra large   \n","2                         never                         2472          small   \n","3                        rarely                           74         medium   \n","4               very frequently                         8457          large   \n","\n","   Waste Bag Weekly Count  How Long TV PC Daily Hour  \\\n","0                       4                          7   \n","1                       3                          9   \n","2                       1                         14   \n","3                       3                         20   \n","4                       1                          3   \n","\n","   How Many New Clothes Monthly  How Long Internet Daily Hour  \\\n","0                            26                             1   \n","1                            38                             5   \n","2                            47                             6   \n","3                             5                             7   \n","4                             5                             6   \n","\n","  Energy efficiency                               Recycling  \\\n","0                No                               ['Metal']   \n","1                No                               ['Metal']   \n","2         Sometimes                               ['Metal']   \n","3         Sometimes  ['Paper', 'Plastic', 'Glass', 'Metal']   \n","4               Yes                               ['Paper']   \n","\n","                         Cooking_With  CarbonEmission  \n","0                   ['Stove', 'Oven']            2238  \n","1              ['Stove', 'Microwave']            1892  \n","2               ['Oven', 'Microwave']            2595  \n","3  ['Microwave', 'Grill', 'Airfryer']            1074  \n","4                            ['Oven']            4743  "]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","import numpy as np\n","import seaborn as sns\n","\n","# Change display settings to show all columns\n","pd.set_option('display.max_columns', None)\n","\n","df=pd.read_csv('Carbon_Emission.csv',sep=\";\")\n","df.head()"]},{"cell_type":"markdown","id":"7b0377c8","metadata":{},"source":["# variables \"Transport\" and \"Vehicle Type\" (they have combined information)\n"]},{"cell_type":"code","execution_count":2,"id":"d690324d","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Transport</th>\n","      <th>Vehicle Type</th>\n","      <th>Transport Vehicle Type</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>public</td>\n","      <td>NaN</td>\n","      <td>public</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>walk/bicycle</td>\n","      <td>NaN</td>\n","      <td>walk/bicycle</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>private</td>\n","      <td>petrol</td>\n","      <td>petrol</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>walk/bicycle</td>\n","      <td>NaN</td>\n","      <td>walk/bicycle</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>private</td>\n","      <td>diesel</td>\n","      <td>diesel</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>9995</th>\n","      <td>private</td>\n","      <td>hybrid</td>\n","      <td>hybrid</td>\n","    </tr>\n","    <tr>\n","      <th>9996</th>\n","      <td>private</td>\n","      <td>lpg</td>\n","      <td>lpg</td>\n","    </tr>\n","    <tr>\n","      <th>9997</th>\n","      <td>walk/bicycle</td>\n","      <td>NaN</td>\n","      <td>walk/bicycle</td>\n","    </tr>\n","    <tr>\n","      <th>9998</th>\n","      <td>private</td>\n","      <td>petrol</td>\n","      <td>petrol</td>\n","    </tr>\n","    <tr>\n","      <th>9999</th>\n","      <td>private</td>\n","      <td>electric</td>\n","      <td>electric</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>10000 rows × 3 columns</p>\n","</div>"],"text/plain":["         Transport Vehicle Type Transport Vehicle Type\n","0           public          NaN                 public\n","1     walk/bicycle          NaN           walk/bicycle\n","2          private       petrol                 petrol\n","3     walk/bicycle          NaN           walk/bicycle\n","4          private       diesel                 diesel\n","...            ...          ...                    ...\n","9995       private       hybrid                 hybrid\n","9996       private          lpg                    lpg\n","9997  walk/bicycle          NaN           walk/bicycle\n","9998       private       petrol                 petrol\n","9999       private     electric               electric\n","\n","[10000 rows x 3 columns]"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["\n","##create new column: \n","df[\"Transport Vehicle Type\"]=df[\"Vehicle Type\"] #definiere neue Spalte\n","df.loc[df[\"Transport Vehicle Type\"].isna(), \"Transport Vehicle Type\"] = df[\"Transport\"] # Werte aus 'Transport' übernehmen, wenn 'Vehicle Type' NaN ist\n","\n","\n","##veranschaulichen der neuen Spalten und ihrer Werte\n","df[[\"Transport\",\"Vehicle Type\",\"Transport Vehicle Type\"]]\n"]},{"cell_type":"markdown","id":"024ca70d","metadata":{},"source":["# variables \"Recycling\" and \"Cooking_With\" (these variables allow multiple answers)"]},{"cell_type":"code","execution_count":4,"id":"28e43b75","metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Recycling</th>\n","      <th>Cooking_With</th>\n","      <th>CarbonEmission</th>\n","      <th>Transport Vehicle Type</th>\n","      <th>Recycling Plastic</th>\n","      <th>Recycling Paper</th>\n","      <th>Recycling Glass</th>\n","      <th>Recycling Metal</th>\n","      <th>Cooking With Oven</th>\n","      <th>Cooking With Grill</th>\n","      <th>Cooking With Microwave</th>\n","      <th>Cooking With Stove</th>\n","      <th>Cooking With Airfryer</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>282</th>\n","      <td>['Paper', 'Plastic', 'Metal']</td>\n","      <td>[]</td>\n","      <td>1484</td>\n","      <td>public</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>283</th>\n","      <td>[]</td>\n","      <td>['Stove', 'Grill', 'Airfryer']</td>\n","      <td>2955</td>\n","      <td>public</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                         Recycling                    Cooking_With  \\\n","282  ['Paper', 'Plastic', 'Metal']                              []   \n","283                             []  ['Stove', 'Grill', 'Airfryer']   \n","\n","     CarbonEmission Transport Vehicle Type  Recycling Plastic  \\\n","282            1484                 public                  1   \n","283            2955                 public                  0   \n","\n","     Recycling Paper  Recycling Glass  Recycling Metal  Cooking With Oven  \\\n","282                1                0                1                  0   \n","283                0                0                0                  0   \n","\n","     Cooking With Grill  Cooking With Microwave  Cooking With Stove  \\\n","282                   0                       0                   0   \n","283                   1                       0                   1   \n","\n","     Cooking With Airfryer  \n","282                      0  \n","283                      1  "]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["\n","##create new column: dummy-variables for Recycling\n","unique_values_Recycling= set([item for sublist in df['Recycling'].unique() for item in eval(sublist)]) # Liste aller einzigartigen Recycling-Möglichkeiten\n","# Für jede einzigartige Recycling-Möglichkeit eine neue Spalte hinzufügen und mit 0 oder 1 füllen\n","for item in unique_values_Recycling:\n","    df['Recycling '+str(item)] = df['Recycling'].apply(lambda x: 1 if item in x else 0)\n","\n","##create new column: dummy-variables for Cooking-with\n","unique_values_cooking_With= set([item for sublist in df['Cooking_With'].unique() for item in eval(sublist)]) # Liste aller einzigartigen Cooking_With-Möglichkeiten\n","# Für jede einzigartige Cooking_With-Möglichkeit eine neue Spalte hinzufügen und mit 0 oder 1 füllen\n","for item in unique_values_cooking_With:\n","    df['Cooking With '+str(item)] = df['Cooking_With'].apply(lambda x: 1 if item in x else 0)\n","\n","\n","#Person 282 cooks with \"nothing\" deshalb can bei one-hot-encoding nicht eine Spalte gelöscht werden, person 283 who doesn't recycle deshalb can bei one-hot-encoding nicht eine Spalte gelöscht werden\n","#Darstellung \n","df.iloc[282:284, -13:] "]},{"cell_type":"markdown","id":"6f8e8336","metadata":{},"source":["# sorting the variable into groups "]},{"cell_type":"code","execution_count":5,"id":"2c2ffabd","metadata":{},"outputs":[],"source":["for_image_processing=['Body Type'\n",",'Sex'\n",",'Diet'\n",",'Heating Energy Source'\n",",'Transport'\n",",'Vehicle Type'\n",",'Waste Bag Size'\n",",'Waste Bag Weekly Count'\n",",'How Many New Clothes Monthly'\n",",'Energy efficiency'\n",",'Recycling'\n",",'Cooking_With']\n","\n","\n","variables_quantitative=df[for_image_processing].select_dtypes(include=[np.number]).columns.tolist()\n","\n","variables_for_one_hot_encoded=list(set(df[for_image_processing].select_dtypes(include=['object','category']).columns.tolist()) - {'Transport','Vehicle Type','Recycling','Cooking_With'})\n","\n","\n","##add \"Transport Vehicle Type\" to \"one-hot-encoding\"-list\n","if \"Transport Vehicle Type\" not in variables_for_one_hot_encoded: ##wenn es noch nicht hinzugefügt wurde bzw is one element in the list already?\n","    variables_for_one_hot_encoded.append(\"Transport Vehicle Type\") \n","\n","\n","##add dummy-varialbes for 'Recycling' and 'Cooking With' to \"variables_quantitative\"-list for Regression\n","columns_recycling=['Recycling '+str(item) for item in unique_values_Recycling] #liste mit Spalten-Namen\n","if columns_recycling[0] not in variables_quantitative: ##wenn es noch nicht hinzugefügt wurde bzw is one element in the list already?\n","    variables_quantitative += columns_recycling\n","columns_cooking_with=['Cooking With '+str(item) for item in unique_values_cooking_With] #liste mit Spalten-Namen\n","if columns_cooking_with[0] not in variables_quantitative: ##wenn es noch nicht hinzugefügt wurde bzw is one element in the list already?\n","    variables_quantitative += columns_cooking_with\n"]},{"cell_type":"markdown","id":"f4cfc449","metadata":{},"source":["### One-Hot-Encoding for categorical variables"]},{"cell_type":"code","execution_count":6,"id":"994324a5","metadata":{},"outputs":[],"source":["#one-hot-encoding\n","\n","from sklearn.preprocessing import OneHotEncoder\n","from sklearn.compose import ColumnTransformer\n","\n","X = df[variables_quantitative + variables_for_one_hot_encoded]   #Ziel: every column except \"CarbonEmission\"\n","\n","# Create the ColumnTransformer using the list \"variables_for_one_hot_encoded\"\n","cf = ColumnTransformer(\n","    [(col, OneHotEncoder(drop=\"first\"), [col]) for col in variables_for_one_hot_encoded],  \n","    remainder=\"passthrough\")   \n","\n","\n","cf.fit(X)\n","X_transformed = cf.transform(X)"]},{"cell_type":"markdown","id":"307f5b9d","metadata":{},"source":["#  Split the data into training and testing sets"]},{"cell_type":"code","execution_count":7,"id":"7eedd6ba","metadata":{},"outputs":[],"source":["\n","y = df[\"CarbonEmission\"]\n","\n","\n","    ##Splitting the dataset into train and test set\n","from sklearn.model_selection import train_test_split \n","X_train, X_test, y_train, y_test = train_test_split(X_transformed,y, train_size = 0.75)\n"]},{"cell_type":"markdown","id":"5ebbaeaa","metadata":{},"source":["# model for image-processing"]},{"cell_type":"code","execution_count":8,"id":"153db044","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:755: ConvergenceWarning: Early stopping the lars path, as the residues are small and the current value of alpha is no longer well controlled. 30 iterations, alpha=6.293e+00, previous alpha=6.218e+00, with an active set of 29 regressors.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.385e+01, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.088e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.326e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.071e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 21 iterations, i.e. alpha=1.385e+01, with an active set of 21 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 28 iterations, i.e. alpha=6.088e+00, with an active set of 28 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.326e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_least_angle.py:725: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 30 iterations, i.e. alpha=1.071e+00, with an active set of 30 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.\n","  warnings.warn(\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_huber.py:342: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res, self.max_iter)\n","/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/linear_model/_glm/glm.py:283: ConvergenceWarning: lbfgs failed to converge (status=1):\n","STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n","\n","Increase the number of iterations (max_iter) or scale the data as shown in:\n","    https://scikit-learn.org/stable/modules/preprocessing.html\n","  self.n_iter_ = _check_optimize_result(\"lbfgs\", opt_res)\n"]},{"name":"stdout","output_type":"stream","text":["[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000625 seconds.\n","You can set `force_col_wise=true` to remove the overhead.\n","[LightGBM] [Info] Total Bins 119\n","[LightGBM] [Info] Number of data points in the train set: 7500, number of used features: 32\n","[LightGBM] [Info] Start training from score 2272.083733\n"]},{"name":"stderr","output_type":"stream","text":["/opt/homebrew/Caskroom/miniforge/base/envs/elisabeth_python-umgebung_nr1/lib/python3.11/site-packages/sklearn/neural_network/_multilayer_perceptron.py:690: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n","  warnings.warn(\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Model</th>\n","      <th>R2 Score</th>\n","      <th>mean absolute error</th>\n","      <th>Train Score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>8</th>\n","      <td>BayesianRidge</td>\n","      <td>0.557090</td>\n","      <td>540.402039</td>\n","      <td>0.569117</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>RidgeCV</td>\n","      <td>0.557040</td>\n","      <td>540.379347</td>\n","      <td>0.569134</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Ridge</td>\n","      <td>0.557040</td>\n","      <td>540.379347</td>\n","      <td>0.569134</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>Linear Regression</td>\n","      <td>0.556985</td>\n","      <td>540.363856</td>\n","      <td>0.569139</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>LassoLarsIC</td>\n","      <td>0.556984</td>\n","      <td>540.364474</td>\n","      <td>0.569139</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>LassoLarsCV</td>\n","      <td>0.556984</td>\n","      <td>540.364474</td>\n","      <td>0.569139</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Lars</td>\n","      <td>0.556984</td>\n","      <td>540.364474</td>\n","      <td>0.569139</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>LarsCV</td>\n","      <td>0.556984</td>\n","      <td>540.364474</td>\n","      <td>0.569139</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Lasso</td>\n","      <td>0.556589</td>\n","      <td>540.746594</td>\n","      <td>0.568811</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>PoissonRegressor</td>\n","      <td>0.555338</td>\n","      <td>539.610343</td>\n","      <td>0.573409</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>LassoCV</td>\n","      <td>0.553916</td>\n","      <td>542.559239</td>\n","      <td>0.566293</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>HuberRegressor</td>\n","      <td>0.549090</td>\n","      <td>540.429703</td>\n","      <td>0.555726</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>HistGradientBoostingRegressor</td>\n","      <td>0.541483</td>\n","      <td>543.018235</td>\n","      <td>0.701465</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>LGBMRegressor</td>\n","      <td>0.541435</td>\n","      <td>543.049766</td>\n","      <td>0.701465</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>GradientBoostingRegressor</td>\n","      <td>0.538651</td>\n","      <td>545.637600</td>\n","      <td>0.575569</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>SGDRegressor</td>\n","      <td>0.527639</td>\n","      <td>561.395316</td>\n","      <td>0.545663</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>PassiveAggressiveRegressor</td>\n","      <td>0.513695</td>\n","      <td>575.928419</td>\n","      <td>0.525293</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>RandomForestRegressor</td>\n","      <td>0.490453</td>\n","      <td>572.860684</td>\n","      <td>0.928017</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>ExtraTreesRegressor</td>\n","      <td>0.452968</td>\n","      <td>584.644192</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>MLPRegressor</td>\n","      <td>0.448333</td>\n","      <td>597.881710</td>\n","      <td>0.461959</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>BaggingRegressor</td>\n","      <td>0.436681</td>\n","      <td>595.872520</td>\n","      <td>0.898460</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>KernelRidge</td>\n","      <td>0.376984</td>\n","      <td>630.356689</td>\n","      <td>0.385276</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>OrthogonalMatchingPursuitCV</td>\n","      <td>0.305636</td>\n","      <td>657.543026</td>\n","      <td>0.340644</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>AdaBoostRegressor</td>\n","      <td>0.271082</td>\n","      <td>717.650322</td>\n","      <td>0.312582</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>ElasticNet</td>\n","      <td>0.240018</td>\n","      <td>678.454855</td>\n","      <td>0.250265</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>KNeighborsRegressor</td>\n","      <td>0.174765</td>\n","      <td>698.384800</td>\n","      <td>0.448726</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>TweedieRegressor</td>\n","      <td>0.170063</td>\n","      <td>704.960503</td>\n","      <td>0.177546</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>GammaRegressor</td>\n","      <td>0.169804</td>\n","      <td>700.271669</td>\n","      <td>0.187381</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>ElasticNetCV</td>\n","      <td>0.100462</td>\n","      <td>730.448359</td>\n","      <td>0.103644</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>NuSVR</td>\n","      <td>0.024727</td>\n","      <td>740.432375</td>\n","      <td>0.020899</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>SVR</td>\n","      <td>0.003611</td>\n","      <td>735.588917</td>\n","      <td>-0.002005</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>DummyRegressor</td>\n","      <td>-0.000135</td>\n","      <td>770.317600</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>RANSACRegressor</td>\n","      <td>-0.007334</td>\n","      <td>803.602716</td>\n","      <td>0.033050</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>DecisionTreeRegressor</td>\n","      <td>-0.012250</td>\n","      <td>781.889200</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>ExtraTreeRegressor</td>\n","      <td>-0.031814</td>\n","      <td>787.184800</td>\n","      <td>1.000000</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>GaussianProcessRegressor</td>\n","      <td>-1.654522</td>\n","      <td>1323.839489</td>\n","      <td>1.000000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                            Model  R2 Score  mean absolute error  Train Score\n","8                   BayesianRidge  0.557090           540.402039     0.569117\n","7                         RidgeCV  0.557040           540.379347     0.569134\n","9                           Ridge  0.557040           540.379347     0.569134\n","0               Linear Regression  0.556985           540.363856     0.569139\n","3                     LassoLarsIC  0.556984           540.364474     0.569139\n","4                     LassoLarsCV  0.556984           540.364474     0.569139\n","5                            Lars  0.556984           540.364474     0.569139\n","6                          LarsCV  0.556984           540.364474     0.569139\n","14                          Lasso  0.556589           540.746594     0.568811\n","25               PoissonRegressor  0.555338           539.610343     0.573409\n","2                         LassoCV  0.553916           542.559239     0.566293\n","11                 HuberRegressor  0.549090           540.429703     0.555726\n","24  HistGradientBoostingRegressor  0.541483           543.018235     0.701465\n","26                  LGBMRegressor  0.541435           543.049766     0.701465\n","21      GradientBoostingRegressor  0.538651           545.637600     0.575569\n","10                   SGDRegressor  0.527639           561.395316     0.545663\n","32     PassiveAggressiveRegressor  0.513695           575.928419     0.525293\n","19          RandomForestRegressor  0.490453           572.860684     0.928017\n","22            ExtraTreesRegressor  0.452968           584.644192     1.000000\n","28                   MLPRegressor  0.448333           597.881710     0.461959\n","20               BaggingRegressor  0.436681           595.872520     0.898460\n","35                    KernelRidge  0.376984           630.356689     0.385276\n","31    OrthogonalMatchingPursuitCV  0.305636           657.543026     0.340644\n","23              AdaBoostRegressor  0.271082           717.650322     0.312582\n","13                     ElasticNet  0.240018           678.454855     0.250265\n","27            KNeighborsRegressor  0.174765           698.384800     0.448726\n","12               TweedieRegressor  0.170063           704.960503     0.177546\n","29                 GammaRegressor  0.169804           700.271669     0.187381\n","1                    ElasticNetCV  0.100462           730.448359     0.103644\n","18                          NuSVR  0.024727           740.432375     0.020899\n","17                            SVR  0.003611           735.588917    -0.002005\n","34                 DummyRegressor -0.000135           770.317600     0.000000\n","30                RANSACRegressor -0.007334           803.602716     0.033050\n","15          DecisionTreeRegressor -0.012250           781.889200     1.000000\n","16             ExtraTreeRegressor -0.031814           787.184800     1.000000\n","33       GaussianProcessRegressor -1.654522          1323.839489     1.000000"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["from sklearn.model_selection import train_test_split\n","from sklearn.metrics import r2_score, mean_absolute_error\n","from sklearn.linear_model import LinearRegression, ElasticNetCV, LassoCV, LassoLarsIC, LassoLarsCV, Lars, LarsCV, RidgeCV, BayesianRidge, Ridge, SGDRegressor, HuberRegressor, TweedieRegressor, ElasticNet, Lasso\n","from sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\n","from sklearn.svm import SVR, NuSVR, LinearSVR\n","from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, GradientBoostingRegressor, ExtraTreesRegressor, AdaBoostRegressor, HistGradientBoostingRegressor\n","from xgboost import XGBRegressor\n","from lightgbm import LGBMRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.neural_network import MLPRegressor\n","from sklearn.linear_model import PoissonRegressor, GammaRegressor, RANSACRegressor, SGDRegressor\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.compose import TransformedTargetRegressor\n","#from sklearn.linear_model import LinearSVR\n","from sklearn.linear_model import HuberRegressor\n","from sklearn.linear_model import TweedieRegressor\n","#from sklearn.linear_model import GeneralizedLinearRegressor \n","from sklearn.dummy import DummyRegressor\n","from sklearn.linear_model import OrthogonalMatchingPursuitCV, PassiveAggressiveRegressor\n","from sklearn.gaussian_process import GaussianProcessRegressor\n","from sklearn.kernel_ridge import KernelRidge\n","\n","# Initialize all the regressors\n","models = {\n","    \"Linear Regression\": LinearRegression(),\n","    \"ElasticNetCV\": ElasticNetCV(),\n","    \"LassoCV\": LassoCV(),\n","    \"LassoLarsIC\": LassoLarsIC(),\n","    \"LassoLarsCV\": LassoLarsCV(),\n","    \"Lars\": Lars(),\n","    \"LarsCV\": LarsCV(),\n","    \"RidgeCV\": RidgeCV(),\n","    \"BayesianRidge\": BayesianRidge(),\n","    \"Ridge\": Ridge(),\n","    \"SGDRegressor\": SGDRegressor(),\n","    \"HuberRegressor\": HuberRegressor(),\n","    \"TweedieRegressor\": TweedieRegressor(),\n","    \"ElasticNet\": ElasticNet(),\n","    \"Lasso\": Lasso(),\n","    \"DecisionTreeRegressor\": DecisionTreeRegressor(),\n","    \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n","    \"SVR\": SVR(),\n","    \"NuSVR\": NuSVR(),\n","    #\"LinearSVR\": LinearSVR(),\n","    \"RandomForestRegressor\": RandomForestRegressor(),\n","    \"BaggingRegressor\": BaggingRegressor(),\n","    \"GradientBoostingRegressor\": GradientBoostingRegressor(),\n","    \"ExtraTreesRegressor\": ExtraTreesRegressor(),\n","    \"AdaBoostRegressor\": AdaBoostRegressor(),\n","    \"HistGradientBoostingRegressor\": HistGradientBoostingRegressor(),\n","    \"PoissonRegressor\": PoissonRegressor(),\n","    \"LGBMRegressor\": LGBMRegressor(),\n","    \"KNeighborsRegressor\": KNeighborsRegressor(),\n","    \"MLPRegressor\": MLPRegressor(),\n","    \"GammaRegressor\": GammaRegressor(),\n","    \"RANSACRegressor\": RANSACRegressor(),\n","    \"OrthogonalMatchingPursuitCV\": OrthogonalMatchingPursuitCV(),\n","    \"PassiveAggressiveRegressor\": PassiveAggressiveRegressor(),\n","    \"GaussianProcessRegressor\": GaussianProcessRegressor(),\n","    #\"OrthogonalMatchingPursuit\": OrthogonalMatchingPursuit(),\n","    \"ExtraTreeRegressor\": ExtraTreeRegressor(),\n","    \"DummyRegressor\": DummyRegressor(),\n","    #\"LassoLars\": LassoLars(),\n","    \"KernelRidge\": KernelRidge()\n","}\n","\n","# Fit all the models and evaluate\n","results = []\n","for name, model in models.items():\n","    try:\n","        model.fit(X_train, y_train)\n","        y_pred = model.predict(X_test)\n","        r2 = r2_score(y_test, y_pred)  #r2 = round(r2_score(y_test, y_pred),5)\n","        mae = mean_absolute_error(y_test, y_pred)\n","        train_score = model.score(X_train, y_train)\n","        results.append((name, r2, mae, train_score))\n","    except Exception as e:\n","        results.append((name, \"Error\", str(e), \"Error\"))\n","\n","# Create a dataframe to display results\n","results_df = pd.DataFrame(results, columns=['Model', 'R2 Score', 'mean absolute error', 'Train Score'])\n","results_df.sort_values(by='R2 Score', ascending=False, inplace=True) #sort by R2-Score\n","results_df"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"}},"nbformat":4,"nbformat_minor":5}
